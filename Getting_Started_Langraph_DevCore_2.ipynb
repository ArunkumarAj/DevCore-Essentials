{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Chatbots With Langgraph"
      ],
      "metadata": {
        "id": "BRGZBA60Bo3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langgraph langsmith"
      ],
      "metadata": {
        "id": "ZCANZS2rBppr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### To Get LangSmith api_key :: https://smith.langchain.com/"
      ],
      "metadata": {
        "id": "z6rVsXTZEi5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "groq_api_key=userdata.get('groq_api_key')\n",
        "langsmith=userdata.get('LANGSMITH_API_KEY')\n",
        "print(langsmith)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-KUnEWJB3Bm",
        "outputId": "c69bafff-436a-411a-aa7f-ecb7688b3456"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is to tracking the every input and other information in Langsmith portal under \"CourseLanggraph\"\n",
        "import os\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = langsmith\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"]=\"CourseLanggraph\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "vm5ZjinZBvqF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq"
      ],
      "metadata": {
        "id": "TFKgf708E5Ra"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"Gemma2-9b-It\")\n",
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uEAFkebFOna",
        "outputId": "deb608aa-4168-4a34-a63a-c15f7f014375"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7a8ab4ba51d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7a8ab4ba7850>, model_name='Gemma2-9b-It', model_kwargs={}, groq_api_key=SecretStr('**********'))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start Building Chatbot Using Langgraph\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UTdcJ3keJ4oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "from typing_extensions import TypedDict\n",
        "from langgraph.graph import StateGraph,START,END\n",
        "from langgraph.graph.message import add_messages"
      ],
      "metadata": {
        "id": "DM4vlPA_FQdM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class State(TypedDict):\n",
        "  # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "  messages:Annotated[list,add_messages]\n",
        "\n",
        "graph_builder=StateGraph(State)\n"
      ],
      "metadata": {
        "id": "CL_1z3DOJ7OF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_builder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K60O_ndBJ-K3",
        "outputId": "e2534ce0-4523-4f6a-bdfc-800ee6f5c49f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7a8ab4aaffd0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(state:State):\n",
        "  return {\"messages\":llm.invoke(state['messages'])}"
      ],
      "metadata": {
        "id": "sub6qk0tKBBs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_builder.add_node(\"chatbot\",chatbot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp2AYUi6KC7V",
        "outputId": "c494b460-1b07-4173-8d96-f849a1fb2089"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7a8ab4aaffd0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph_builder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KntSZhFKF6H",
        "outputId": "383d96dd-0494-4774-b84a-b37b8830f24e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7a8ab4aaffd0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph_builder.add_edge(START,\"chatbot\")\n",
        "graph_builder.add_edge(\"chatbot\",END)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHdQS7B_KLxR",
        "outputId": "a0aac147-3d93-4f1f-b3cb-482d2d1cab11"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7a8ab4aaffd0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph=graph_builder.compile()"
      ],
      "metadata": {
        "id": "UbU0rTcLKP-7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "try:\n",
        "  display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "  pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "uBDthEBpKSHO",
        "outputId": "9b0a0b6f-2863-4f14-9df2-cd82c440452e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "  user_input=input(\"User: \")\n",
        "  if user_input.lower() in [\"quit\",\"q\"]:\n",
        "    print(\"Good Bye\")\n",
        "    break\n",
        "  for event in graph.stream({'messages':(\"user\",user_input)}):\n",
        "    print(event.values())\n",
        "    for value in event.values():\n",
        "      print(value['messages'])\n",
        "      print(\"Assistant:\",value[\"messages\"].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGP30vG2KUnG",
        "outputId": "5938a1de-ed86-4333-ca78-def82498f56a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: Hi\n",
            "dict_values([{'messages': AIMessage(content='Hi! 👋 \\n\\nWhat can I do for you today? 😊\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 10, 'total_tokens': 27, 'completion_time': 0.030909091, 'prompt_time': 4.2e-07, 'queue_time': 0.055723194999999996, 'total_time': 0.030909511}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-79f135db-32cf-42f3-bbf3-7ada971aadd4-0', usage_metadata={'input_tokens': 10, 'output_tokens': 17, 'total_tokens': 27})}])\n",
            "content='Hi! 👋 \\n\\nWhat can I do for you today? 😊\\n' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 10, 'total_tokens': 27, 'completion_time': 0.030909091, 'prompt_time': 4.2e-07, 'queue_time': 0.055723194999999996, 'total_time': 0.030909511}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-79f135db-32cf-42f3-bbf3-7ada971aadd4-0' usage_metadata={'input_tokens': 10, 'output_tokens': 17, 'total_tokens': 27}\n",
            "Assistant: Hi! 👋 \n",
            "\n",
            "What can I do for you today? 😊\n",
            "\n",
            "User: tell me the steps to follow that to train my own LLM Model in my Commercial GPU 3050Ti. start from data collecting, data preprocessing, data labelling, spliting if requred, training using perfect good model and so on till the deployment!\n",
            "dict_values([{'messages': AIMessage(content=\"Training your own LLM on a commercial GPU like the 3050 Ti is a challenging but achievable task. Here's a breakdown of the steps involved:\\n\\n**1. Data Collection & Curation**\\n\\n* **Define your domain:** What kind of tasks do you want your LLM to excel at? (e.g., code generation, creative writing, question answering). This will guide your data selection.\\n* **Gather data:** \\n    * **Public datasets:** Explore repositories like HuggingFace Datasets, Common Crawl, and OpenWebText for relevant text data.\\n    * **Web scraping:** Carefully and ethically scrape data from websites related to your domain. Respect robots.txt and terms of service.\\n    * **Internal data:** If applicable, utilize your own company's documents, emails, or other text-based resources.\\n* **Clean and filter:** Remove duplicates, irrelevant information (HTML tags, special characters), personal data (PII), and offensive content.\\n\\n**2. Data Preprocessing**\\n\\n* **Tokenization:** Break down text into smaller units (words or subwords) using a tokenizer like BPE or WordPiece. This prepares the text for the model.\\n* **Normalization:**  \\n    * Convert text to lowercase.\\n    * Handle contractions and abbreviations.\\n    * Standardize punctuation.\\n* **Encoding:**  Represent tokens as numerical vectors using techniques like word embeddings (Word2Vec, GloVe) or learn embeddings directly during training.\\n\\n**3. Data Labeling (Optional)**\\n\\n* **Supervised learning:** If you want your LLM to perform specific tasks (e.g., sentiment analysis, text classification), you'll need labeled data. This means assigning categories or tags to your text examples.\\n* **Unsupervised learning:** For more general-purpose LLMs, labeling might not be necessary.\\n\\n**4. Data Splitting**\\n\\n* **Training set:** The largest portion (e.g., 80%) used to train the model.\\n* **Validation set:** A smaller portion (e.g., 10%) used to monitor performance during training and tune hyperparameters.\\n* **Test set:** A separate portion (e.g., 10%) used to evaluate the final model's performance on unseen data.\\n\\n**5. Choosing a Model Architecture**\\n\\n* **Transformer-based:** Architectures like GPT, BERT, and T5 are highly effective for LLMs.\\n* **Size matters:**  The 3050 Ti has 8GB of VRAM, which limits the size of models you can train effectively. Start with smaller models and scale up if needed.\\n\\n**6. Training**\\n\\n* **Framework:** Use deep learning frameworks like PyTorch or TensorFlow.\\n* **Hyperparameter tuning:** Experiment with learning rate, batch size, number of epochs, and other settings to find the best configuration.\\n* **Monitor progress:** Track the model's performance on the validation set to prevent overfitting.\\n* **Save checkpoints:** Regularly save your model's weights during training to avoid losing progress.\\n\\n**7. Evaluation & Testing**\\n\\n* **Evaluate on test set:** Assess the model's performance using relevant metrics (e.g., perplexity, accuracy, BLEU score) on the held-out test data.\\n* **Analyze results:** Identify strengths and weaknesses of the model.\\n\\n**8. Deployment**\\n\\n* **Cloud hosting:** Services like AWS, Google Cloud, or Azure offer scalable infrastructure for deploying LLMs.\\n* **On-premise server:** If you have the resources, host the model on your own server.\\n* **API:** Create an API to allow other applications to interact with your LLM.\\n\\n**Important Considerations:**\\n\\n* **Computational resources:** Training LLMs is computationally intensive. Even with a 3050 Ti, it will take time and potentially require cloud computing for larger models.\\n* **Dataset size:** The quality and quantity of your training data significantly impact the model's performance.\\n* **Ethical implications:** Be mindful of biases in your data and the potential misuse of your LLM.\\n\\n**Remember:** Training an LLM is an iterative process. You'll likely need to experiment with different datasets, architectures, and training techniques to achieve satisfactory results.\\n\\n\\nLet me know if you have questions about any of these steps.\\n\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 909, 'prompt_tokens': 63, 'total_tokens': 972, 'completion_time': 1.652727273, 'prompt_time': 0.001859016, 'queue_time': 0.055136332, 'total_time': 1.654586289}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-103ddf50-d387-4535-888f-71483ed0e976-0', usage_metadata={'input_tokens': 63, 'output_tokens': 909, 'total_tokens': 972})}])\n",
            "content=\"Training your own LLM on a commercial GPU like the 3050 Ti is a challenging but achievable task. Here's a breakdown of the steps involved:\\n\\n**1. Data Collection & Curation**\\n\\n* **Define your domain:** What kind of tasks do you want your LLM to excel at? (e.g., code generation, creative writing, question answering). This will guide your data selection.\\n* **Gather data:** \\n    * **Public datasets:** Explore repositories like HuggingFace Datasets, Common Crawl, and OpenWebText for relevant text data.\\n    * **Web scraping:** Carefully and ethically scrape data from websites related to your domain. Respect robots.txt and terms of service.\\n    * **Internal data:** If applicable, utilize your own company's documents, emails, or other text-based resources.\\n* **Clean and filter:** Remove duplicates, irrelevant information (HTML tags, special characters), personal data (PII), and offensive content.\\n\\n**2. Data Preprocessing**\\n\\n* **Tokenization:** Break down text into smaller units (words or subwords) using a tokenizer like BPE or WordPiece. This prepares the text for the model.\\n* **Normalization:**  \\n    * Convert text to lowercase.\\n    * Handle contractions and abbreviations.\\n    * Standardize punctuation.\\n* **Encoding:**  Represent tokens as numerical vectors using techniques like word embeddings (Word2Vec, GloVe) or learn embeddings directly during training.\\n\\n**3. Data Labeling (Optional)**\\n\\n* **Supervised learning:** If you want your LLM to perform specific tasks (e.g., sentiment analysis, text classification), you'll need labeled data. This means assigning categories or tags to your text examples.\\n* **Unsupervised learning:** For more general-purpose LLMs, labeling might not be necessary.\\n\\n**4. Data Splitting**\\n\\n* **Training set:** The largest portion (e.g., 80%) used to train the model.\\n* **Validation set:** A smaller portion (e.g., 10%) used to monitor performance during training and tune hyperparameters.\\n* **Test set:** A separate portion (e.g., 10%) used to evaluate the final model's performance on unseen data.\\n\\n**5. Choosing a Model Architecture**\\n\\n* **Transformer-based:** Architectures like GPT, BERT, and T5 are highly effective for LLMs.\\n* **Size matters:**  The 3050 Ti has 8GB of VRAM, which limits the size of models you can train effectively. Start with smaller models and scale up if needed.\\n\\n**6. Training**\\n\\n* **Framework:** Use deep learning frameworks like PyTorch or TensorFlow.\\n* **Hyperparameter tuning:** Experiment with learning rate, batch size, number of epochs, and other settings to find the best configuration.\\n* **Monitor progress:** Track the model's performance on the validation set to prevent overfitting.\\n* **Save checkpoints:** Regularly save your model's weights during training to avoid losing progress.\\n\\n**7. Evaluation & Testing**\\n\\n* **Evaluate on test set:** Assess the model's performance using relevant metrics (e.g., perplexity, accuracy, BLEU score) on the held-out test data.\\n* **Analyze results:** Identify strengths and weaknesses of the model.\\n\\n**8. Deployment**\\n\\n* **Cloud hosting:** Services like AWS, Google Cloud, or Azure offer scalable infrastructure for deploying LLMs.\\n* **On-premise server:** If you have the resources, host the model on your own server.\\n* **API:** Create an API to allow other applications to interact with your LLM.\\n\\n**Important Considerations:**\\n\\n* **Computational resources:** Training LLMs is computationally intensive. Even with a 3050 Ti, it will take time and potentially require cloud computing for larger models.\\n* **Dataset size:** The quality and quantity of your training data significantly impact the model's performance.\\n* **Ethical implications:** Be mindful of biases in your data and the potential misuse of your LLM.\\n\\n**Remember:** Training an LLM is an iterative process. You'll likely need to experiment with different datasets, architectures, and training techniques to achieve satisfactory results.\\n\\n\\nLet me know if you have questions about any of these steps.\\n\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 909, 'prompt_tokens': 63, 'total_tokens': 972, 'completion_time': 1.652727273, 'prompt_time': 0.001859016, 'queue_time': 0.055136332, 'total_time': 1.654586289}, 'model_name': 'Gemma2-9b-It', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None} id='run-103ddf50-d387-4535-888f-71483ed0e976-0' usage_metadata={'input_tokens': 63, 'output_tokens': 909, 'total_tokens': 972}\n",
            "Assistant: Training your own LLM on a commercial GPU like the 3050 Ti is a challenging but achievable task. Here's a breakdown of the steps involved:\n",
            "\n",
            "**1. Data Collection & Curation**\n",
            "\n",
            "* **Define your domain:** What kind of tasks do you want your LLM to excel at? (e.g., code generation, creative writing, question answering). This will guide your data selection.\n",
            "* **Gather data:** \n",
            "    * **Public datasets:** Explore repositories like HuggingFace Datasets, Common Crawl, and OpenWebText for relevant text data.\n",
            "    * **Web scraping:** Carefully and ethically scrape data from websites related to your domain. Respect robots.txt and terms of service.\n",
            "    * **Internal data:** If applicable, utilize your own company's documents, emails, or other text-based resources.\n",
            "* **Clean and filter:** Remove duplicates, irrelevant information (HTML tags, special characters), personal data (PII), and offensive content.\n",
            "\n",
            "**2. Data Preprocessing**\n",
            "\n",
            "* **Tokenization:** Break down text into smaller units (words or subwords) using a tokenizer like BPE or WordPiece. This prepares the text for the model.\n",
            "* **Normalization:**  \n",
            "    * Convert text to lowercase.\n",
            "    * Handle contractions and abbreviations.\n",
            "    * Standardize punctuation.\n",
            "* **Encoding:**  Represent tokens as numerical vectors using techniques like word embeddings (Word2Vec, GloVe) or learn embeddings directly during training.\n",
            "\n",
            "**3. Data Labeling (Optional)**\n",
            "\n",
            "* **Supervised learning:** If you want your LLM to perform specific tasks (e.g., sentiment analysis, text classification), you'll need labeled data. This means assigning categories or tags to your text examples.\n",
            "* **Unsupervised learning:** For more general-purpose LLMs, labeling might not be necessary.\n",
            "\n",
            "**4. Data Splitting**\n",
            "\n",
            "* **Training set:** The largest portion (e.g., 80%) used to train the model.\n",
            "* **Validation set:** A smaller portion (e.g., 10%) used to monitor performance during training and tune hyperparameters.\n",
            "* **Test set:** A separate portion (e.g., 10%) used to evaluate the final model's performance on unseen data.\n",
            "\n",
            "**5. Choosing a Model Architecture**\n",
            "\n",
            "* **Transformer-based:** Architectures like GPT, BERT, and T5 are highly effective for LLMs.\n",
            "* **Size matters:**  The 3050 Ti has 8GB of VRAM, which limits the size of models you can train effectively. Start with smaller models and scale up if needed.\n",
            "\n",
            "**6. Training**\n",
            "\n",
            "* **Framework:** Use deep learning frameworks like PyTorch or TensorFlow.\n",
            "* **Hyperparameter tuning:** Experiment with learning rate, batch size, number of epochs, and other settings to find the best configuration.\n",
            "* **Monitor progress:** Track the model's performance on the validation set to prevent overfitting.\n",
            "* **Save checkpoints:** Regularly save your model's weights during training to avoid losing progress.\n",
            "\n",
            "**7. Evaluation & Testing**\n",
            "\n",
            "* **Evaluate on test set:** Assess the model's performance using relevant metrics (e.g., perplexity, accuracy, BLEU score) on the held-out test data.\n",
            "* **Analyze results:** Identify strengths and weaknesses of the model.\n",
            "\n",
            "**8. Deployment**\n",
            "\n",
            "* **Cloud hosting:** Services like AWS, Google Cloud, or Azure offer scalable infrastructure for deploying LLMs.\n",
            "* **On-premise server:** If you have the resources, host the model on your own server.\n",
            "* **API:** Create an API to allow other applications to interact with your LLM.\n",
            "\n",
            "**Important Considerations:**\n",
            "\n",
            "* **Computational resources:** Training LLMs is computationally intensive. Even with a 3050 Ti, it will take time and potentially require cloud computing for larger models.\n",
            "* **Dataset size:** The quality and quantity of your training data significantly impact the model's performance.\n",
            "* **Ethical implications:** Be mindful of biases in your data and the potential misuse of your LLM.\n",
            "\n",
            "**Remember:** Training an LLM is an iterative process. You'll likely need to experiment with different datasets, architectures, and training techniques to achieve satisfactory results.\n",
            "\n",
            "\n",
            "Let me know if you have questions about any of these steps.\n",
            "\n",
            "User: q\n",
            "Good Bye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QA7NtQkdKXOh"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}
